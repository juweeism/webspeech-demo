{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, Request, status\n",
    "from fastapi.responses import Response, StreamingResponse\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import uvicorn\n",
    "from pydantic import BaseModel\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import openai\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from openai import OpenAI\n",
    "import ast\n",
    "import json\n",
    "from fastapi.middleware.cors import CORSMiddleware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not len(OPENAI_API_KEY):\n",
    "    print(\"Set OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Speech(BaseModel):\n",
    "    excerpt:str\n",
    "    previous_suggestion:str\n",
    "    topic: str\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "origins = [\n",
    "    \"http://localhost:4321\",\n",
    "]\n",
    "\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=origins,\n",
    "    allow_credentials = True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"]\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_speech(excerpt, previous_suggestion):\n",
    "    prompt = [{\"role\":\"system\",\"content\":'''Your job is to continue my speech in 1 sentence. Your output should be a json with only one key, which is next_sentence. Refer to your previous suggestion. \n",
    "               If user is following your previous suggestion. The value of next_sentence should be the part of the previous suggestion that is not yet included in the excerpt plus a new sentence. Else,\n",
    "               the value of next_sentence should just be a new sentence.'''},\n",
    "              {\"role\":\"user\",\"content\":f\"Previous suggestion: {previous_suggestion}\"},\n",
    "              {\"role\":\"user\",\"content\":f\"Speech so far: {excerpt}\"}]\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.post(\"/generate_speech\",status_code=status.HTTP_202_ACCEPTED)\n",
    "def continue_speech(speech:Speech):\n",
    "    try:\n",
    "        client = OpenAI()\n",
    "        response = client.chat.completions.create(\n",
    "            model = \"gpt-3.5-turbo-0125\",\n",
    "            response_format={\"type\":\"json_object\"},\n",
    "            temperature=0.7,\n",
    "            messages=generate_speech(excerpt=speech.excerpt,previous_suggestion=speech.previous_suggestion)\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(\"ERROR!\")\n",
    "        return {\"Error\":e}\n",
    "\n",
    "# uvicorn.run(app, host=\"192.168.0.23\",port=8080)\n",
    "uvicorn.run(app, host=\"192.168.68.142\",port=8080)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [6656]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://192.168.68.142:8080 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     192.168.68.142:61759 - \"POST /generate_speech_async HTTP/1.1\" 500 Internal Server Error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\LXPH\\anaconda3\\lib\\site-packages\\uvicorn\\protocols\\http\\h11_impl.py\", line 408, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "  File \"c:\\Users\\LXPH\\anaconda3\\lib\\site-packages\\uvicorn\\middleware\\proxy_headers.py\", line 84, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "  File \"c:\\Users\\LXPH\\anaconda3\\lib\\site-packages\\fastapi\\applications.py\", line 289, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"c:\\Users\\LXPH\\anaconda3\\lib\\site-packages\\starlette\\applications.py\", line 122, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"c:\\Users\\LXPH\\anaconda3\\lib\\site-packages\\starlette\\middleware\\errors.py\", line 184, in __call__\n",
      "    raise exc\n",
      "  File \"c:\\Users\\LXPH\\anaconda3\\lib\\site-packages\\starlette\\middleware\\errors.py\", line 162, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"c:\\Users\\LXPH\\anaconda3\\lib\\site-packages\\starlette\\middleware\\cors.py\", line 91, in __call__\n",
      "    await self.simple_response(scope, receive, send, request_headers=headers)\n",
      "  File \"c:\\Users\\LXPH\\anaconda3\\lib\\site-packages\\starlette\\middleware\\cors.py\", line 146, in simple_response\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"c:\\Users\\LXPH\\anaconda3\\lib\\site-packages\\starlette\\middleware\\exceptions.py\", line 79, in __call__\n",
      "    raise exc\n",
      "  File \"c:\\Users\\LXPH\\anaconda3\\lib\\site-packages\\starlette\\middleware\\exceptions.py\", line 68, in __call__\n",
      "    await self.app(scope, receive, sender)\n",
      "  File \"c:\\Users\\LXPH\\anaconda3\\lib\\site-packages\\fastapi\\middleware\\asyncexitstack.py\", line 20, in __call__\n",
      "    raise e\n",
      "  File \"c:\\Users\\LXPH\\anaconda3\\lib\\site-packages\\fastapi\\middleware\\asyncexitstack.py\", line 17, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"c:\\Users\\LXPH\\anaconda3\\lib\\site-packages\\starlette\\routing.py\", line 718, in __call__\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"c:\\Users\\LXPH\\anaconda3\\lib\\site-packages\\starlette\\routing.py\", line 276, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"c:\\Users\\LXPH\\anaconda3\\lib\\site-packages\\starlette\\routing.py\", line 66, in app\n",
      "    response = await func(request)\n",
      "  File \"c:\\Users\\LXPH\\anaconda3\\lib\\site-packages\\fastapi\\routing.py\", line 273, in app\n",
      "    raw_response = await run_endpoint_function(\n",
      "  File \"c:\\Users\\LXPH\\anaconda3\\lib\\site-packages\\fastapi\\routing.py\", line 190, in run_endpoint_function\n",
      "    return await dependant.call(**values)\n",
      "  File \"C:\\Users\\LXPH\\AppData\\Local\\Temp\\ipykernel_6656\\3696757673.py\", line 25, in continue_speech_async\n",
      "    messages = generate_speech_async(excerpt = speech.excerpt, previous_suggestion = speech.previous_suggestion, topic = topic)\n",
      "NameError: name 'topic' is not defined\n",
      "INFO:     Shutting down\n",
      "INFO:     Waiting for application shutdown.\n",
      "INFO:     Application shutdown complete.\n",
      "INFO:     Finished server process [6656]\n"
     ]
    }
   ],
   "source": [
    "def generate_speech_async(excerpt, previous_suggestion,topic):\n",
    "    prompt = [{\"role\":\"system\",\"content\":'''Your job is to continue my speech in 1 sentence. Do not say anything else. \n",
    "    Your response should be string. I read your suggestions, but I can choose not to follow them. If I am not yet done\n",
    "    reading your previous suggestion, output your previous suggestion. Else, output a new sentence to continue my speech.'''},\n",
    "        {\"role\":\"user\",\"content\":f\"Topic: {topic}\"},\n",
    "        {\"role\":\"user\",\"content\":f\"Speech so far: {excerpt}\"},\n",
    "        {\"role\":\"user\",\"content\":f\"Previous suggestion: {previous_suggestion}\"}\n",
    "    ]\n",
    "    return prompt\n",
    "\n",
    "async def standard_stream_generator(response, **kwargs):\n",
    "    msg = \"\"\n",
    "    for chunk in response:\n",
    "        try:\n",
    "            chunk_response = chunk.choices[0].delta.content\n",
    "            msg += chunk_response\n",
    "            print(msg)\n",
    "            yield msg.encode()\n",
    "        except:\n",
    "            yield \"\"\n",
    "            continue\n",
    "\n",
    "@app.post(\"/generate_speech_async\",status_code=status.HTTP_202_ACCEPTED)\n",
    "async def continue_speech_async(speech:Speech):\n",
    "    messages = generate_speech_async(excerpt = speech.excerpt, previous_suggestion = speech.previous_suggestion, topic = speech.topic)\n",
    "    print(messages)\n",
    "    try:\n",
    "        client = OpenAI()\n",
    "        response = client.chat.completions.create(\n",
    "            model = \"gpt-3.5-turbo\",\n",
    "            temperature = 0.7,\n",
    "            messages = messages,\n",
    "            stream = True\n",
    "        )\n",
    "        return StreamingResponse(standard_stream_generator(response,excerpt = speech.excerpt, previous_suggestion = speech.previous_suggestion,\n",
    "                                    topic = speech.topic),\n",
    "                                    media_type='text/event-stream')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return \"\"\n",
    "uvicorn.run(app,host=\"192.168.68.142\",port=8080)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
