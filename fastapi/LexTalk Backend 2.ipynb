{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, Request, status\n",
    "from fastapi.responses import Response\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import uvicorn\n",
    "from pydantic import BaseModel\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import openai\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from openai import OpenAI\n",
    "import ast\n",
    "import json\n",
    "from fastapi.middleware.cors import CORSMiddleware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not len(OPENAI_API_KEY):\n",
    "    print(\"Set OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Speech(BaseModel):\n",
    "    excerpt:str\n",
    "    previous_suggestion:str\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "origins = [\n",
    "    \"http://localhost:4321\",\n",
    "]\n",
    "\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=origins,\n",
    "    allow_credentials = True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"]\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_speech(excerpt, previous_suggestion):\n",
    "    prompt = [{\"role\":\"system\",\"content\":'''Your job is to continue my speech in 1 sentence. Your output should be a json with only one key, which is next_sentence. Refer to your previous suggestion. \n",
    "               If user is following your previous suggestion. The value of next_sentence should be the part of the previous suggestion that is not yet included in the excerpt plus a new sentence. Else,\n",
    "               the value of next_sentence should just be a new sentence.'''},\n",
    "              {\"role\":\"user\",\"content\":f\"Previous suggestion: {previous_suggestion}\"},\n",
    "              {\"role\":\"user\",\"content\":f\"Speech so far: {excerpt}\"}]\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [18564]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://192.168.0.23:8080 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     192.168.0.23:53329 - \"OPTIONS /generate_speech HTTP/1.1\" 200 OK\n",
      "INFO:     192.168.0.23:53329 - \"POST /generate_speech HTTP/1.1\" 422 Unprocessable Entity\n",
      "INFO:     192.168.0.23:53330 - \"POST /generate_speech HTTP/1.1\" 202 Accepted\n",
      "INFO:     192.168.0.23:53340 - \"POST /generate_speech HTTP/1.1\" 202 Accepted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Shutting down\n",
      "INFO:     Waiting for connections to close. (CTRL+C to force quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     192.168.0.23:53343 - \"POST /generate_speech HTTP/1.1\" 202 Accepted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Waiting for application shutdown.\n",
      "INFO:     Application shutdown complete.\n",
      "INFO:     Finished server process [18564]\n"
     ]
    }
   ],
   "source": [
    "@app.post(\"/generate_speech\",status_code=status.HTTP_202_ACCEPTED)\n",
    "def continue_speech(speech:Speech):\n",
    "    try:\n",
    "        client = OpenAI()\n",
    "        response = client.chat.completions.create(\n",
    "            model = \"gpt-3.5-turbo-0125\",\n",
    "            response_format={\"type\":\"json_object\"},\n",
    "            temperature=0.7,\n",
    "            messages=generate_speech(excerpt=speech.excerpt,previous_suggestion=speech.previous_suggestion)\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(\"ERROR!\")\n",
    "        return {\"Error\":e}\n",
    "\n",
    "uvicorn.run(app, host=\"192.168.0.23\",port=8080)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
